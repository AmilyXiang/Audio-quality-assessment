"""
æ·±åº¦æ•°æ®åˆ†æ - æå–å¸§çº§è¯¦ç»†æ•°æ®ã€ç»Ÿè®¡ç‰¹å¾ã€æ—¶é—´åºåˆ—ç­‰ä¿¡æ¯
ç”¨äºLLMæ·±åº¦è§£è¯»ï¼ˆä¸éœ€è¦è§†è§‰åŠŸèƒ½ï¼‰
"""

import json
import os
from pathlib import Path
import numpy as np
from collections import defaultdict

def load_detailed_analysis_data(summary_path, json_dir, num_worst=15, num_best=10):
    """åŠ è½½è¯¦ç»†åˆ†ææ•°æ®"""
    
    # è¯»å–æ±‡æ€»æ•°æ®
    with open(summary_path, 'r', encoding='utf-8') as f:
        summary = json.load(f)
    
    # è·å–æ–‡ä»¶åˆ—è¡¨
    worst_files = summary.get('worst_20_files', [])[:num_worst]
    best_files = summary.get('best_10_files', [])[:num_best]
    
    # æ·±åº¦åˆ†ææ•°æ®ç»“æ„
    deep_analysis = {
        'worst_files_detail': [],
        'best_files_detail': [],
        'time_series_patterns': {},
        'correlation_analysis': {},
        'anomaly_detection': {},
        'statistical_summary': {}
    }
    
    # åˆ†ææœ€å·®æ–‡ä»¶
    print(f"æ­£åœ¨æ·±åº¦åˆ†ææœ€å·®çš„{num_worst}ä¸ªæ–‡ä»¶...")
    for item in worst_files:
        filename = item['filename']
        json_path = Path(json_dir) / f"baseline_compare_{filename}.json"
        
        if json_path.exists():
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
                # æå–è¯¦ç»†æ•°æ®
                detail = analyze_single_file_deeply(filename, data, item)
                deep_analysis['worst_files_detail'].append(detail)
    
    # åˆ†ææœ€å¥½æ–‡ä»¶
    print(f"æ­£åœ¨æ·±åº¦åˆ†ææœ€å¥½çš„{num_best}ä¸ªæ–‡ä»¶...")
    for item in best_files:
        filename = item['filename']
        json_path = Path(json_dir) / f"baseline_compare_{filename}.json"
        
        if json_path.exists():
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                detail = analyze_single_file_deeply(filename, data, item)
                deep_analysis['best_files_detail'].append(detail)
    
    # è·¨æ–‡ä»¶ç»Ÿè®¡åˆ†æ
    print("æ­£åœ¨è¿›è¡Œè·¨æ–‡ä»¶ç»Ÿè®¡åˆ†æ...")
    deep_analysis['statistical_summary'] = compute_cross_file_statistics(
        deep_analysis['worst_files_detail'],
        deep_analysis['best_files_detail']
    )
    
    # æ—¶é—´æ¨¡å¼åˆ†æ
    print("æ­£åœ¨åˆ†ææ—¶é—´æ¨¡å¼...")
    deep_analysis['time_series_patterns'] = analyze_time_patterns(
        deep_analysis['worst_files_detail'] + deep_analysis['best_files_detail']
    )
    
    # å¼‚å¸¸æ£€æµ‹
    print("æ­£åœ¨æ£€æµ‹å¼‚å¸¸æ¨¡å¼...")
    deep_analysis['anomaly_detection'] = detect_anomalies(
        deep_analysis['worst_files_detail']
    )
    
    return summary, deep_analysis

def analyze_single_file_deeply(filename, data, summary_item):
    """æ·±åº¦åˆ†æå•ä¸ªæ–‡ä»¶"""
    
    metrics = data.get('metrics', {})
    file_level = data.get('file_level', {})
    
    # æå–åŸºæœ¬ä¿¡æ¯
    detail = {
        'filename': filename,
        'mos_diff': summary_item.get('mos_diff', 0),
        'mos_below_pct': summary_item.get('mos_below_pct', 0),
        'file_level_scores': file_level,
        'frame_analysis': {},
        'trend_info': {},
        'anomaly_frames': []
    }
    
    # åˆ†æå„ç»´åº¦çš„å¸§çº§æ•°æ®
    for dim in ['mos', 'noi', 'dis', 'col', 'loud']:
        dim_data = metrics.get(dim, {})
        
        if 'frame_diffs' in dim_data:
            frame_diffs = dim_data['frame_diffs']
            diffs = [f['diff'] for f in frame_diffs]
            
            # ç»Ÿè®¡ç‰¹å¾
            detail['frame_analysis'][dim.upper()] = {
                'mean': np.mean(diffs),
                'std': np.std(diffs),
                'min': np.min(diffs),
                'max': np.max(diffs),
                'median': np.median(diffs),
                'q25': np.percentile(diffs, 25),
                'q75': np.percentile(diffs, 75),
                'range': np.max(diffs) - np.min(diffs),
                'variability': np.std(diffs) / (np.mean(np.abs(diffs)) + 1e-6)
            }
            
            # è¶‹åŠ¿ä¿¡æ¯
            trend = dim_data.get('trend', {})
            if trend:
                detail['trend_info'][dim.upper()] = {
                    'type': trend.get('type'),
                    'description': trend.get('description'),
                    'slope': trend.get('slope', 0)
                }
                
                # çªå˜ç‚¹
                sudden_drops = trend.get('sudden_drops', [])
                for drop in sudden_drops:
                    detail['anomaly_frames'].append({
                        'dimension': dim.upper(),
                        'frame': drop.get('frame'),
                        'drop_value': drop.get('drop'),
                        'time': drop.get('frame', 0) * 0.5  # å‡è®¾0.5sæ­¥é•¿
                    })
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats = dim_data.get('stats', {})
        if stats:
            below_pct = stats.get('percent_below_baseline', 0)
            detail['frame_analysis'][dim.upper()]['below_baseline_pct'] = below_pct
    
    # è§£ææ–‡ä»¶åå…ƒæ•°æ®
    detail['metadata'] = parse_filename_metadata(filename)
    
    return detail

def parse_filename_metadata(filename):
    """è§£ææ–‡ä»¶åä¸­çš„å…ƒæ•°æ®"""
    
    metadata = {
        'timestamp': None,
        'ip_address': None,
        'device_code': None,
        'location_code': None
    }
    
    parts = filename.split('_')
    
    # å°è¯•æå–æ—¶é—´æˆ³ï¼ˆå¦‚260206125256ï¼‰
    if len(parts) > 0 and parts[0].isdigit() and len(parts[0]) >= 12:
        timestamp = parts[0]
        metadata['timestamp'] = f"20{timestamp[:2]}-{timestamp[2:4]}-{timestamp[4:6]} {timestamp[6:8]}:{timestamp[8:10]}:{timestamp[10:12]}"
    
    # å°è¯•æå–IPåœ°å€
    for part in parts:
        if '.' in part and all(x.isdigit() or x == '.' for x in part):
            metadata['ip_address'] = part
    
    # æå–è®¾å¤‡ä»£ç ï¼ˆå¦‚BOXP, HFï¼‰
    for part in parts:
        if part in ['BOXP', 'BOXR', 'HF', 'BOX']:
            metadata['device_code'] = part
        if part in ['WHS', 'P']:
            metadata['location_code'] = part
    
    return metadata

def compute_cross_file_statistics(worst_files, best_files):
    """è®¡ç®—è·¨æ–‡ä»¶ç»Ÿè®¡ç‰¹å¾"""
    
    stats = {
        'worst_vs_best_comparison': {},
        'dimension_severity_ranking': [],
        'common_anomaly_times': []
    }
    
    # å¯¹æ¯”æœ€å·®å’Œæœ€å¥½æ–‡ä»¶çš„ç»Ÿè®¡ç‰¹å¾
    for dim in ['MOS', 'NOI', 'DIS', 'COL', 'LOUD']:
        worst_means = [f['frame_analysis'].get(dim, {}).get('mean', 0) 
                      for f in worst_files if dim in f.get('frame_analysis', {})]
        best_means = [f['frame_analysis'].get(dim, {}).get('mean', 0) 
                     for f in best_files if dim in f.get('frame_analysis', {})]
        
        if worst_means and best_means:
            stats['worst_vs_best_comparison'][dim] = {
                'worst_avg': float(np.mean(worst_means)),
                'best_avg': float(np.mean(best_means)),
                'gap': float(np.mean(best_means) - np.mean(worst_means))
            }
    
    # ç»´åº¦ä¸¥é‡æ€§æ’åº
    dimension_severity = []
    for dim in ['MOS', 'NOI', 'DIS', 'COL', 'LOUD']:
        below_pcts = [f['frame_analysis'].get(dim, {}).get('below_baseline_pct', 0) 
                     for f in worst_files if dim in f.get('frame_analysis', {})]
        if below_pcts:
            avg_severity = np.mean(below_pcts)
            dimension_severity.append({
                'dimension': dim,
                'avg_severity': float(avg_severity)
            })
    
    stats['dimension_severity_ranking'] = sorted(
        dimension_severity, 
        key=lambda x: x['avg_severity'], 
        reverse=True
    )
    
    # å¯»æ‰¾å…±åŒçš„å¼‚å¸¸æ—¶é—´ç‚¹
    anomaly_times = defaultdict(int)
    for file_detail in worst_files:
        for anomaly in file_detail.get('anomaly_frames', []):
            time_bucket = int(anomaly['time'] / 2) * 2  # 2ç§’æ—¶é—´æ¡¶
            anomaly_times[time_bucket] += 1
    
    # æ‰¾å‡ºå‡ºç°é¢‘ç‡é«˜çš„å¼‚å¸¸æ—¶é—´ç‚¹
    common_times = [{'time_range': f"{t}-{t+2}s", 'file_count': count} 
                   for t, count in anomaly_times.items() if count >= 3]
    stats['common_anomaly_times'] = sorted(common_times, key=lambda x: x['file_count'], reverse=True)[:5]
    
    return stats

def analyze_time_patterns(all_files):
    """åˆ†ææ—¶é—´æ¨¡å¼"""
    
    patterns = {
        'by_timestamp': defaultdict(list),
        'by_ip': defaultdict(list),
        'by_device': defaultdict(list)
    }
    
    for file_detail in all_files:
        metadata = file_detail.get('metadata', {})
        mos_diff = file_detail.get('mos_diff', 0)
        
        # æŒ‰æ—¶é—´æˆ³åˆ†ç»„
        if metadata.get('timestamp'):
            date = metadata['timestamp'][:10]  # æå–æ—¥æœŸéƒ¨åˆ†
            patterns['by_timestamp'][date].append(mos_diff)
        
        # æŒ‰IPåˆ†ç»„
        if metadata.get('ip_address'):
            patterns['by_ip'][metadata['ip_address']].append(mos_diff)
        
        # æŒ‰è®¾å¤‡åˆ†ç»„
        if metadata.get('device_code'):
            patterns['by_device'][metadata['device_code']].append(mos_diff)
    
    # è®¡ç®—æ¯ç»„çš„å¹³å‡è´¨é‡
    result = {
        'timestamp_quality': {},
        'ip_quality': {},
        'device_quality': {}
    }
    
    for date, diffs in patterns['by_timestamp'].items():
        result['timestamp_quality'][date] = {
            'avg_mos_diff': float(np.mean(diffs)),
            'file_count': len(diffs),
            'quality_level': 'good' if np.mean(diffs) > 0 else 'poor'
        }
    
    for ip, diffs in patterns['by_ip'].items():
        result['ip_quality'][ip] = {
            'avg_mos_diff': float(np.mean(diffs)),
            'file_count': len(diffs),
            'quality_level': 'good' if np.mean(diffs) > 0 else 'poor'
        }
    
    for device, diffs in patterns['by_device'].items():
        result['device_quality'][device] = {
            'avg_mos_diff': float(np.mean(diffs)),
            'file_count': len(diffs),
            'quality_level': 'good' if np.mean(diffs) > 0 else 'poor'
        }
    
    return result

def detect_anomalies(worst_files):
    """æ£€æµ‹å¼‚å¸¸æ¨¡å¼"""
    
    anomalies = {
        'extreme_variability': [],
        'persistent_degradation': [],
        'multi_dimension_failure': []
    }
    
    for file_detail in worst_files:
        filename = file_detail['filename']
        frame_analysis = file_detail.get('frame_analysis', {})
        
        # æ£€æµ‹æç«¯æ³¢åŠ¨ï¼ˆé«˜æ ‡å‡†å·®ï¼‰
        high_var_dims = []
        for dim, stats in frame_analysis.items():
            if stats.get('std', 0) > 0.3:  # æ ‡å‡†å·®é˜ˆå€¼
                high_var_dims.append({
                    'dimension': dim,
                    'std': stats['std'],
                    'variability': stats.get('variability', 0)
                })
        
        if high_var_dims:
            anomalies['extreme_variability'].append({
                'filename': filename,
                'dimensions': high_var_dims
            })
        
        # æ£€æµ‹æŒç»­åŠ£åŒ–ï¼ˆ>70%å¸§ä½äºåŸºå‡†ï¼‰
        persistent_dims = []
        for dim, stats in frame_analysis.items():
            below_pct = stats.get('below_baseline_pct', 0)
            if below_pct > 70:
                persistent_dims.append({
                    'dimension': dim,
                    'below_pct': below_pct
                })
        
        if persistent_dims:
            anomalies['persistent_degradation'].append({
                'filename': filename,
                'dimensions': persistent_dims
            })
        
        # æ£€æµ‹å¤šç»´åº¦åŒæ—¶å¤±è´¥ï¼ˆ>=3ä¸ªç»´åº¦>50%åŠ£åŒ–ï¼‰
        failed_dims = []
        for dim, stats in frame_analysis.items():
            below_pct = stats.get('below_baseline_pct', 0)
            if below_pct > 50:
                failed_dims.append(dim)
        
        if len(failed_dims) >= 3:
            anomalies['multi_dimension_failure'].append({
                'filename': filename,
                'failed_dimensions': failed_dims,
                'failure_count': len(failed_dims)
            })
    
    return anomalies

def format_deep_analysis_prompt(summary, deep_analysis):
    """æ ¼å¼åŒ–æ·±åº¦åˆ†ææç¤º"""
    
    prompt = f"""# éŸ³é¢‘è´¨é‡æ·±åº¦æ•°æ®åˆ†æä»»åŠ¡

åŸºäºNISQAè´¨é‡è¯„ä¼°å·¥å…·ï¼Œæˆ‘å·²æå–äº†è¯¦å°½çš„å¸§çº§æ•°æ®ã€ç»Ÿè®¡ç‰¹å¾å’Œæ—¶é—´æ¨¡å¼ï¼Œè¯·æ‚¨ä½œä¸ºéŸ³é¢‘è´¨é‡ä¸“å®¶è¿›è¡Œæ·±åº¦åˆ†æã€‚

## æ•°æ®é›†æ¦‚å†µ

- **æ€»æ–‡ä»¶æ•°**: {summary['total_files']}
- **è´¨é‡åˆ†å¸ƒ**: è‰¯å¥½{summary['quality_distribution']['good']}ä¸ª, ä¸­ç­‰åŠ£åŒ–{summary['quality_distribution']['moderate']}ä¸ª, ä¸¥é‡åŠ£åŒ–{summary['quality_distribution']['severe']}ä¸ª

---

## ç¬¬ä¸€éƒ¨åˆ†ï¼šæœ€å·®æ–‡ä»¶å¸§çº§è¯¦ç»†åˆ†æ

æˆ‘æå–äº†{len(deep_analysis['worst_files_detail'])}ä¸ªè´¨é‡æœ€å·®æ–‡ä»¶çš„è¯¦ç»†æ•°æ®ï¼š

"""
    
    # æœ€å·®æ–‡ä»¶è¯¦æƒ…
    for i, file_detail in enumerate(deep_analysis['worst_files_detail'][:5], 1):
        prompt += f"""
### {i}. {file_detail['filename']}

**åŸºæœ¬æŒ‡æ ‡**:
- MOSæ–‡ä»¶çº§å·®å€¼: {file_detail['mos_diff']:.3f}
- MOSä½äºåŸºå‡†å¸§å æ¯”: {file_detail['mos_below_pct']:.1f}%

**å…ƒæ•°æ®è§£æ**:
"""
        metadata = file_detail['metadata']
        if metadata.get('timestamp'):
            prompt += f"- å½•åˆ¶æ—¶é—´: {metadata['timestamp']}\n"
        if metadata.get('ip_address'):
            prompt += f"- è®¾å¤‡IP: {metadata['ip_address']}\n"
        if metadata.get('device_code'):
            prompt += f"- è®¾å¤‡ä»£å·: {metadata['device_code']}\n"
        
        prompt += "\n**å¸§çº§ç»Ÿè®¡ç‰¹å¾**:\n"
        for dim, stats in file_detail['frame_analysis'].items():
            prompt += f"- {dim}: å‡å€¼{stats['mean']:.3f}, æ ‡å‡†å·®{stats['std']:.3f}, èŒƒå›´[{stats['min']:.3f}, {stats['max']:.3f}], ä½äºåŸºå‡†{stats.get('below_baseline_pct', 0):.1f}%\n"
        
        # è¶‹åŠ¿
        if file_detail['trend_info']:
            prompt += "\n**è´¨é‡è¶‹åŠ¿**:\n"
            for dim, trend in file_detail['trend_info'].items():
                prompt += f"- {dim}: {trend['description']} (æ–œç‡={trend['slope']:.4f})\n"
        
        # å¼‚å¸¸å¸§
        if file_detail['anomaly_frames']:
            prompt += f"\n**æ£€æµ‹åˆ°{len(file_detail['anomaly_frames'])}ä¸ªè´¨é‡çªé™ç‚¹**:\n"
            for anomaly in file_detail['anomaly_frames'][:3]:
                prompt += f"- {anomaly['dimension']} åœ¨ {anomaly['time']:.1f}ç§’å¤„ä¸‹é™{anomaly['drop_value']:.3f}\n"
    
    # è·¨æ–‡ä»¶ç»Ÿè®¡
    prompt += f"""

---

## ç¬¬äºŒéƒ¨åˆ†ï¼šæœ€å·® vs æœ€å¥½æ–‡ä»¶å¯¹æ¯”

ç»´åº¦å¹³å‡å·®å€¼å¯¹æ¯”ï¼ˆæ­£å€¼è¡¨ç¤ºæœ€å¥½æ–‡ä»¶æ›´ä¼˜ï¼‰:
"""
    
    comparison = deep_analysis['statistical_summary'].get('worst_vs_best_comparison', {})
    for dim, comp_data in comparison.items():
        prompt += f"- {dim}: æœ€å·®ç»„={comp_data['worst_avg']:.3f}, æœ€å¥½ç»„={comp_data['best_avg']:.3f}, å·®è·={comp_data['gap']:.3f}\n"
    
    # ç»´åº¦ä¸¥é‡æ€§æ’åº
    prompt += "\n**ç»´åº¦é—®é¢˜ä¸¥é‡æ€§æ’åº** (æŒ‰å¹³å‡åŠ£åŒ–å¸§ç™¾åˆ†æ¯”):\n"
    for rank in deep_analysis['statistical_summary'].get('dimension_severity_ranking', []):
        prompt += f"{rank['dimension']}: {rank['avg_severity']:.1f}%  "
    
    # å…±åŒå¼‚å¸¸æ—¶é—´
    common_anomalies = deep_analysis['statistical_summary'].get('common_anomaly_times', [])
    if common_anomalies:
        prompt += "\n\n**å…±åŒå¼‚å¸¸æ—¶é—´æ®µ** (å¤šä¸ªæ–‡ä»¶åœ¨æ­¤æ—¶æ®µåŒæ—¶è´¨é‡ä¸‹é™):\n"
        for anomaly in common_anomalies:
            prompt += f"- {anomaly['time_range']}: {anomaly['file_count']}ä¸ªæ–‡ä»¶å‡ºç°å¼‚å¸¸\n"
    
    # æ—¶é—´æ¨¡å¼åˆ†æ
    prompt += """

---

## ç¬¬ä¸‰éƒ¨åˆ†ï¼šæ—¶é—´ã€è®¾å¤‡ã€ç½‘ç»œæ¨¡å¼åˆ†æ

"""
    
    time_patterns = deep_analysis['time_series_patterns']
    
    # æŒ‰æ—¶é—´
    prompt += "**æŒ‰å½•åˆ¶æ—¥æœŸç»Ÿè®¡**:\n"
    timestamp_quality = time_patterns.get('timestamp_quality', {})
    for date, info in sorted(timestamp_quality.items()):
        prompt += f"- {date}: {info['file_count']}ä¸ªæ–‡ä»¶, å¹³å‡MOSå·®å€¼={info['avg_mos_diff']:.3f}, è´¨é‡={info['quality_level']}\n"
    
    # æŒ‰IP
    prompt += "\n**æŒ‰è®¾å¤‡IPç»Ÿè®¡**:\n"
    ip_quality = time_patterns.get('ip_quality', {})
    for ip, info in sorted(ip_quality.items(), key=lambda x: x[1]['avg_mos_diff']):
        prompt += f"- {ip}: {info['file_count']}ä¸ªæ–‡ä»¶, å¹³å‡MOSå·®å€¼={info['avg_mos_diff']:.3f}, è´¨é‡={info['quality_level']}\n"
    
    # æŒ‰è®¾å¤‡
    prompt += "\n**æŒ‰è®¾å¤‡ç±»å‹ç»Ÿè®¡**:\n"
    device_quality = time_patterns.get('device_quality', {})
    for device, info in sorted(device_quality.items(), key=lambda x: x[1]['avg_mos_diff']):
        prompt += f"- {device}: {info['file_count']}ä¸ªæ–‡ä»¶, å¹³å‡MOSå·®å€¼={info['avg_mos_diff']:.3f}, è´¨é‡={info['quality_level']}\n"
    
    # å¼‚å¸¸æ¨¡å¼æ£€æµ‹
    prompt += """

---

## ç¬¬å››éƒ¨åˆ†ï¼šå¼‚å¸¸æ¨¡å¼æ£€æµ‹ç»“æœ

"""
    
    anomalies = deep_analysis['anomaly_detection']
    
    # æç«¯æ³¢åŠ¨
    extreme_var = anomalies.get('extreme_variability', [])
    if extreme_var:
        prompt += f"**æç«¯æ³¢åŠ¨æ–‡ä»¶** ({len(extreme_var)}ä¸ªï¼Œæ ‡å‡†å·®>0.3):\n"
        for item in extreme_var[:3]:
            dims = ', '.join([f"{d['dimension']}(std={d['std']:.3f})" for d in item['dimensions']])
            prompt += f"- {item['filename']}: {dims}\n"
    
    # æŒç»­åŠ£åŒ–
    persistent = anomalies.get('persistent_degradation', [])
    if persistent:
        prompt += f"\n**æŒç»­åŠ£åŒ–æ–‡ä»¶** ({len(persistent)}ä¸ªï¼Œ>70%å¸§ä½äºåŸºå‡†):\n"
        for item in persistent[:3]:
            dims = ', '.join([f"{d['dimension']}({d['below_pct']:.0f}%)" for d in item['dimensions']])
            prompt += f"- {item['filename']}: {dims}\n"
    
    # å¤šç»´åº¦å¤±è´¥
    multi_fail = anomalies.get('multi_dimension_failure', [])
    if multi_fail:
        prompt += f"\n**å¤šç»´åº¦åŒæ—¶å¤±è´¥** ({len(multi_fail)}ä¸ªï¼Œ>=3ç»´åº¦>50%åŠ£åŒ–):\n"
        for item in multi_fail[:3]:
            prompt += f"- {item['filename']}: {len(item['failed_dimensions'])}ä¸ªç»´åº¦å¤±è´¥ ({', '.join(item['failed_dimensions'])})\n"
    
    # åˆ†æä»»åŠ¡
    prompt += """

---

## è¯·æ‚¨æ·±åº¦åˆ†æå¹¶å›ç­”ï¼š

### 1. å¸§çº§æ•°æ®æ´å¯Ÿ
- ä»å¸§çº§ç»Ÿè®¡ç‰¹å¾ï¼ˆå‡å€¼ã€æ ‡å‡†å·®ã€èŒƒå›´ã€åˆ†ä½æ•°ï¼‰ä¸­ï¼Œæ‚¨å‘ç°äº†ä»€ä¹ˆè´¨é‡ç‰¹å¾ï¼Ÿ
- å“ªäº›æ–‡ä»¶çš„æ³¢åŠ¨æ€§ï¼ˆæ ‡å‡†å·®/variabilityï¼‰æœ€å¤§ï¼Ÿè¿™åæ˜ äº†ä»€ä¹ˆé—®é¢˜ï¼Ÿ
- è´¨é‡è¶‹åŠ¿åˆ†æä¸­ï¼Œ"æ”¹å–„è¶‹åŠ¿"å’Œ"æ¶åŒ–è¶‹åŠ¿"åˆ†åˆ«æ„å‘³ç€ä»€ä¹ˆï¼Ÿ

### 2. ç»´åº¦ç›¸å…³æ€§ä¸ä¼˜å…ˆçº§
- äº”ä¸ªç»´åº¦ï¼ˆMOS/NOI/DIS/COL/LOUDï¼‰ä¹‹é—´æ˜¯å¦å­˜åœ¨ç›¸å…³æ€§ï¼Ÿ
- å“ªä¸ªç»´åº¦æ˜¯"ä¸»å¯¼å› ç´ "ï¼ˆæ”¹å–„å®ƒå¯¹æ•´ä½“è´¨é‡æå‡æœ€å¤§ï¼‰ï¼Ÿ
- ä¸ºä»€ä¹ˆNOIï¼ˆå™ªå£°ï¼‰çš„é—®é¢˜æœ€ä¸¥é‡ï¼Ÿ

### 3. æ—¶é—´ã€è®¾å¤‡ã€ç½‘ç»œæ¨¡å¼æ·±åº¦è§£è¯»
- åŸºäºæ—¶é—´ç»Ÿè®¡ï¼Œå“ªä¸ªæ—¥æœŸæ˜¯"è´¨é‡äº‹æ•…æ—¥"ï¼Ÿå¯èƒ½å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ
- å“ªä¸ªIPåœ°å€çš„è®¾å¤‡è´¨é‡æœ€å·®ï¼Ÿåº”è¯¥å¦‚ä½•æ’æŸ¥ï¼Ÿ
- ä¸åŒè®¾å¤‡ä»£å·ï¼ˆBOXP/BOXR/HFï¼‰ä¹‹é—´çš„è´¨é‡å·®å¼‚è¯´æ˜äº†ä»€ä¹ˆï¼Ÿ

### 4. å¼‚å¸¸å¸§ä¸çªå˜ç‚¹åˆ†æ
- å¤šä¸ªæ–‡ä»¶åœ¨ç›¸åŒæ—¶é—´æ®µå‡ºç°å¼‚å¸¸ï¼Œè¿™æ˜¯å·§åˆè¿˜æ˜¯ç³»ç»Ÿæ€§é—®é¢˜ï¼Ÿ
- è´¨é‡çªé™ï¼ˆsudden dropsï¼‰æœ€å¸¸å‘ç”Ÿåœ¨éŸ³é¢‘çš„å“ªä¸ªé˜¶æ®µï¼ˆå¼€å¤´/ä¸­é—´/ç»“å°¾ï¼‰ï¼Ÿ
- å¦‚ä½•è§£é‡Š"æŒç»­åŠ£åŒ–"ä¸"æç«¯æ³¢åŠ¨"çš„åŒºåˆ«å’ŒåŸå› ï¼Ÿ

### 5. æ ¹æœ¬åŸå› å‡è®¾ä¸éªŒè¯æ–¹æ¡ˆ
- åŸºäºæ‰€æœ‰æ•°æ®ï¼Œæ‚¨å¯¹è´¨é‡é—®é¢˜çš„æ ¹æœ¬åŸå› æœ‰ä»€ä¹ˆå‡è®¾ï¼Ÿ
- å¦‚ä½•éªŒè¯è¿™äº›å‡è®¾ï¼ˆéœ€è¦æ”¶é›†å“ªäº›é¢å¤–æ•°æ®æˆ–æ—¥å¿—ï¼‰ï¼Ÿ
- æå‡º3-5ä¸ªå¯ç«‹å³æ‰§è¡Œçš„æ”¹è¿›æªæ–½ï¼ŒæŒ‰ROIæ’åºã€‚

### 6. é¢„æµ‹æ€§ç»´æŠ¤å»ºè®®
- åŸºäºæ•°æ®æ¨¡å¼ï¼Œå¦‚ä½•é¢„æµ‹ä¸‹ä¸€æ¬¡è´¨é‡é—®é¢˜å¯èƒ½åœ¨ä½•æ—¶ä½•åœ°å‘ç”Ÿï¼Ÿ
- åº”è¯¥å»ºç«‹å“ªäº›å®æ—¶ç›‘æ§æŒ‡æ ‡å’Œå‘Šè­¦é˜ˆå€¼ï¼Ÿ

è¯·æä¾›ä¸“ä¸šã€æ·±å…¥ä¸”åŸºäºæ•°æ®çš„åˆ†ææŠ¥å‘Šã€‚
"""
    
    return prompt

def call_llm_for_deep_analysis(prompt, api_config):
    """è°ƒç”¨LLMè¿›è¡Œæ·±åº¦åˆ†æ"""
    
    import requests
    
    api_key = api_config.get('api_key')
    model = api_config.get('model', 'deepseek-chat')
    base_url = api_config.get('base_url', 'https://api.deepseek.com/v1')
    
    headers = {
        'Authorization': f'Bearer {api_key}',
        'Content-Type': 'application/json'
    }
    
    data = {
        'model': model,
        'messages': [
            {
                'role': 'system',
                'content': 'ä½ æ˜¯ä¸€ä½èµ„æ·±çš„éŸ³é¢‘è´¨é‡åˆ†æä¸“å®¶ï¼Œç²¾é€šä¿¡å·å¤„ç†ã€ç»Ÿè®¡åˆ†æå’Œæ•°æ®æŒ–æ˜ã€‚ä½ èƒ½å¤Ÿä»å¤æ‚çš„å¸§çº§æ•°æ®ä¸­å‘ç°æ·±å±‚æ¨¡å¼ï¼Œå¹¶æä¾›å¯æ“ä½œçš„æŠ€æœ¯æ´å¯Ÿã€‚'
            },
            {
                'role': 'user',
                'content': prompt
            }
        ],
        'temperature': 0.7,
        'max_tokens': 8000  # æ›´é•¿çš„è¾“å‡º
    }
    
    response = requests.post(
        f'{base_url}/chat/completions',
        headers=headers,
        json=data,
        timeout=180
    )
    
    response.raise_for_status()
    result = response.json()
    
    return result['choices'][0]['message']['content']

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='æ·±åº¦æ•°æ®åˆ†æ - ä½¿ç”¨LLMè§£è¯»è¯¦ç»†æ•°æ®')
    parser.add_argument('--summary', default='quality_summary.json')
    parser.add_argument('--json-dir', default='.')
    parser.add_argument('--output', default='deep_data_analysis_report.md')
    parser.add_argument('--config', default='llm_config.json')
    
    args = parser.parse_args()
    
    print("=" * 80)
    print("NISQAè´¨é‡æ·±åº¦æ•°æ®åˆ†æ - LLMæ™ºèƒ½è§£è¯»")
    print("=" * 80)
    
    # åŠ è½½å’Œåˆ†ææ•°æ®
    print("\nğŸ“Š æ­£åœ¨åŠ è½½å¹¶æ·±åº¦åˆ†ææ•°æ®...")
    summary, deep_analysis = load_detailed_analysis_data(
        args.summary,
        args.json_dir,
        num_worst=15,
        num_best=10
    )
    
    # æ ¼å¼åŒ–æç¤º
    print("\nğŸ“ æ­£åœ¨å‡†å¤‡æ·±åº¦åˆ†ææç¤º...")
    prompt = format_deep_analysis_prompt(summary, deep_analysis)
    
    # åŠ è½½APIé…ç½®
    print("\nğŸ”§ æ­£åœ¨åŠ è½½LLMé…ç½®...")
    
    if not os.path.exists(args.config):
        print(f"\nâŒ é”™è¯¯: é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
        return
    
    with open(args.config, 'r', encoding='utf-8') as f:
        api_config = json.load(f)
    
    print(f"  - æ¨¡å‹: {api_config.get('model')}")
    
    # è°ƒç”¨LLM
    print("\nğŸ¤– æ­£åœ¨è°ƒç”¨LLMè¿›è¡Œæ·±åº¦åˆ†æï¼ˆè¿™å¯èƒ½éœ€è¦2-3åˆ†é’Ÿï¼‰...")
    
    try:
        analysis_report = call_llm_for_deep_analysis(prompt, api_config)
        
        # ç»„åˆå®Œæ•´æŠ¥å‘Š
        full_report = f"""# NISQAéŸ³é¢‘è´¨é‡æ·±åº¦æ•°æ®åˆ†ææŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {__import__('datetime').datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**åˆ†æå·¥å…·**: NISQA + LLMæ·±åº¦æ•°æ®åˆ†æ
**åˆ†ææ¨¡å‹**: {api_config.get('model')}
**æ•°æ®é›†**: {summary['total_files']}ä¸ªéŸ³é¢‘æ–‡ä»¶
**åˆ†ææ–‡ä»¶**: {len(deep_analysis['worst_files_detail'])}ä¸ªæœ€å·® + {len(deep_analysis['best_files_detail'])}ä¸ªæœ€å¥½

---

{analysis_report}

---

## é™„å½•ï¼šæ•°æ®æ¥æºè¯´æ˜

æœ¬æŠ¥å‘ŠåŸºäºä»¥ä¸‹è¯¦ç»†æ•°æ®ï¼š
1. **å¸§çº§ç»Ÿè®¡**: æ¯ä¸ªæ–‡ä»¶çš„5ç»´åº¦å¸§å·®å€¼åˆ†å¸ƒï¼ˆå‡å€¼ã€æ ‡å‡†å·®ã€åˆ†ä½æ•°ã€èŒƒå›´ç­‰ï¼‰
2. **è¶‹åŠ¿åˆ†æ**: çº¿æ€§å›å½’æ–œç‡ã€è¶‹åŠ¿ç±»å‹ï¼ˆç¨³å®š/æ”¹å–„/æ¶åŒ–ï¼‰
3. **å¼‚å¸¸æ£€æµ‹**: è´¨é‡çªé™ç‚¹ã€æç«¯æ³¢åŠ¨ã€æŒç»­åŠ£åŒ–ã€å¤šç»´åº¦å¤±è´¥
4. **æ—¶é—´æ¨¡å¼**: æŒ‰æ—¥æœŸã€IPåœ°å€ã€è®¾å¤‡ç±»å‹åˆ†ç»„çš„è´¨é‡ç»Ÿè®¡
5. **è·¨æ–‡ä»¶å¯¹æ¯”**: æœ€å·®ä¸æœ€å¥½æ–‡ä»¶çš„ç»Ÿè®¡ç‰¹å¾å¯¹æ¯”

### æ•°æ®æ–‡ä»¶
- è¯¦ç»†æ•°æ®: baseline_compare_*.json (å…±{summary['total_files']}ä¸ª)
- å¯è§†åŒ–å›¾è¡¨: baseline_compare_all.png, baseline_compare_heatmap.png
- æ•°æ®æ±‡æ€»: quality_summary.json

---
*æœ¬æŠ¥å‘Šç”±NISQAè´¨é‡è¯„ä¼°å·¥å…·ç»“åˆå¤§è¯­è¨€æ¨¡å‹æ·±åº¦æ•°æ®åˆ†æè‡ªåŠ¨ç”Ÿæˆ*
"""
        
        # ä¿å­˜æŠ¥å‘Š
        with open(args.output, 'w', encoding='utf-8') as f:
            f.write(full_report)
        
        print(f"\nâœ… æ·±åº¦åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {args.output}")
        
        print("\n" + "=" * 80)
        print("âœ… åˆ†æå®Œæˆï¼")
        print(f"ğŸ“„ å®Œæ•´æŠ¥å‘Š: {args.output}")
        print("=" * 80)
        
    except Exception as e:
        print(f"\nâŒ åˆ†æè¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == '__main__':
    main()
